{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| IEEE AI Workshop: Self-Supervised Learning and JEPA in Autonomous Driving\n",
    "\n",
    "\n",
    "## TCP Overview:\n",
    "\n",
    "\n",
    "![teaser](assets/teaser_.png)\n",
    "\n",
    "> Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline  \n",
    "> [Penghao Wu*](https://scholar.google.com/citations?user=9mssd5EAAAAJ&hl=en), [Xiaosong Jia*](https://jiaxiaosong1002.github.io/), [Li Chen*](https://scholar.google.com/citations?user=ulZxvY0AAAAJ&hl=en), [Junchi Yan](https://thinklab.sjtu.edu.cn/), [Hongyang Li](https://lihongyang.info/), [Yu Qiao](http://mmlab.siat.ac.cn/yuqiao/)    \n",
    ">  - [arXiv Paper](https://arxiv.org/abs/2206.08129), NeurIPS 2022\n",
    ">  - [Blog in Chinese](https://zhuanlan.zhihu.com/p/532665469)\n",
    "\n",
    "\n",
    "\n",
    "## Environment Setup\n",
    "Before diving into the practical exercises, ensure all participants have their environments set up correctly.\n",
    "\n",
    "\n",
    "For Training:\n",
    "```bash\n",
    "conda create -f conda_env/tcp_trainer.yml --name TCPTrainer\n",
    "```\n",
    "\n",
    "For Evaluation:\n",
    "```bash\n",
    "conda create -f conda_env/tcp_runner.yml --name TCPEval\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environments\n",
    "!conda create -f conda_env/tcp_trainer.yml --name TCPTrainer\n",
    "!conda create -f conda_env/tcp_runner.yml --name TCPEval\n",
    "\n",
    "# Install the necessary package for the Jepa encoder\n",
    "!pip install vjepa-encoder\n",
    "\n",
    "# Clone the necessary repository for running examples\n",
    "!git clone https://huggingface.co/jonathanzkoch/vjepa-self-driving vjepa/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V-JEPA: A Vision Transformer pretrained on a bunch of video data that achieves SOTA when finetuned on downstream tasks\n",
    "\n",
    "<img src=\"https://github.com/facebookresearch/jepa/assets/7530871/72df7ef0-2ef5-48bb-be46-27963db91f3d\" width=40%>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "\n",
    "<img src=\"https://github.com/facebookresearch/jepa/assets/7530871/f26b2e96-0227-44e2-b058-37e7bf1e10db\" width=40%>\n",
    "\n",
    "\n",
    "### Using V-Jepa:\n",
    "\n",
    " 1. Install the pip package (which we already did)\n",
    " 2. take an image and pass it through the network! (thats it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rpal/anaconda3/envs/jepa/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vjepa/params-encoder.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1113165/2556893716.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Note Required: Update the yaml to update /path/to/vjepa/insert/MODEL_NAME.tar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m encoder = JepaEncoder.load_model(\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m\"vjepa/params-encoder.yaml\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jepa/lib/python3.7/site-packages/vjepa_encoder/vision_encoder.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(cls, config_file_path, device)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0my_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loaded params...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vjepa/params-encoder.yaml'"
     ]
    }
   ],
   "source": [
    "from vjepa_encoder.vision_encoder import JepaEncoder\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "# Note Required: Update the yaml to update /path/to/vjepa/insert/MODEL_NAME.tar\n",
    "encoder = JepaEncoder.load_model(\n",
    "    \"vjepa/params-encoder.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize a random image:\n",
    "img = numpy.random.random(size=(360, 480, 3))\n",
    "\n",
    "\n",
    "print(\"Input Img:\", img.shape)\n",
    "embedding = encoder.embed_image(img)\n",
    "\n",
    "print(embedding)\n",
    "print(embedding.shape)\n",
    "\n",
    "# Initialize another random image:\n",
    "x = torch.rand((32, 3, 256, 900))\n",
    "\n",
    "embedding = encoder.embed_image(x)\n",
    "print(embedding)\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training TCP with JEPA\n",
    "\n",
    "1. Configure the data. This can be downloaded from here: https://drive.usercontent.google.com/download?id=1HZxlSZ_wUVWkNTWMXXcSQxtYdT7GogSm&export=download&authuser=0&confirm=t&uuid=3bbfcf3e-2be2-4aff-94c0-8d6e41ac91de&at=APZUnTXwtDK4X3Vdq_R30bHy58Vi%3A1712945448485 \n",
    "\n",
    "2. (Already done for you) Modify the existing computer vision architecture to use the JEPA architecture\n",
    "\n",
    "3. Run the training script (its that easy!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "main() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# You may want to dynamically adjust paths or other settings here\u001b[39;00m\n\u001b[1;32m     16\u001b[0m args\u001b[38;5;241m.\u001b[39mlogdir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mlogdir, args\u001b[38;5;241m.\u001b[39mid)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: main() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "from TCP.train import main\n",
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    id='TCP',\n",
    "    epochs=10,\n",
    "    lr=0.0001,\n",
    "    val_every=2,\n",
    "    batch_size=64,\n",
    "    logdir='log',\n",
    "    gpus=1\n",
    ")\n",
    "\n",
    "# You may want to dynamically adjust paths or other settings here\n",
    "args.logdir = os.path.join(args.logdir, args.id)\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Eval:\n",
    "To run the eval, you will need to configure Carla. I am running Ubuntu, and you can download this from the GitHub Fork I created:\n",
    "\n",
    "1. Set the python path to include Carla:\n",
    "```bash\n",
    "export PYTHONPATH=$PYTHONPATH:$(pwd)\n",
    "```\n",
    "\n",
    "2. Make sure you update the `TEAM_CONFIG` variable in the eval script to point to the checkpoint file.\n",
    "\n",
    "3. Run the eval script (its that easy)\n",
    "```bash\n",
    "bash ./leaderboard/scripts/run_evaluation.sh \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "!export PYTHONPATH=$PYTHONPATH:$(pwd)\n",
    "!bash ./leaderboard/scripts/run_evaluation.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCP_JEPA: Trajectory and Control Prediction using JEPA Encoder\n",
    "\n",
    "TCP_JEPA is a neural network module designed for trajectory and control prediction in autonomous driving systems. It utilizes the JEPA (Joint Encoding for Prediction and Alignment) encoder for perception and combines it with various components for trajectory and control prediction.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The TCP_JEPA module takes an image, state information, and a target point as input and generates predictions for trajectory and control. It consists of the following main components:\n",
    "\n",
    "1. **Perception**: The perception component uses the JEPA encoder to extract features from the input image. The JEPA encoder is loaded from a configuration file and wrapped in a `PerceptWrapper` class.\n",
    "\n",
    "2. **Attentive Pooler**: The attentive pooler is used to pool the features extracted by the JEPA encoder. It uses a single query and applies attention to obtain a compact representation of the image features.\n",
    "\n",
    "3. **Measurements**: The measurements component processes the state information, which includes speed, position, and other relevant measurements. It applies a series of linear layers and activation functions to transform the state information into a feature representation.\n",
    "\n",
    "4. **Trajectory Prediction**: The trajectory prediction component combines the image features and measurement features to predict the future trajectory of the vehicle. It uses a GRU (Gated Recurrent Unit) cell to generate the trajectory autoregressively.\n",
    "\n",
    "5. **Control Prediction**: The control prediction component combines the image features and measurement features to predict the future control actions of the vehicle. It uses a GRU cell to generate the control actions autoregressively.\n",
    "\n",
    "6. **Output Branches**: The module includes several output branches for predicting speed, value (for both trajectory and control), and the parameters of the control distribution (mean and standard deviation).\n",
    "\n",
    "## Data Shapes\n",
    "\n",
    "The TCP_JEPA module expects the following input shapes:\n",
    "\n",
    "- `img`: The input image tensor with shape `(batch_size, channels, height, width)`.\n",
    "- `state`: The state information tensor with shape `(batch_size, state_dim)`, where `state_dim` is the dimensionality of the state information (1+2+6 in the provided code).\n",
    "- `target_point`: The target point tensor with shape `(batch_size, 2)`, representing the target position.\n",
    "\n",
    "The module generates various output tensors with the following shapes:\n",
    "\n",
    "- `pred_speed`: The predicted speed tensor with shape `(batch_size, 1)`.\n",
    "- `pred_value_traj`: The predicted value for trajectory with shape `(batch_size, 1)`.\n",
    "- `pred_features_traj`: The predicted features for trajectory with shape `(batch_size, feature_dim)`.\n",
    "- `pred_wp`: The predicted waypoints tensor with shape `(batch_size, pred_len, 2)`, where `pred_len` is the length of the predicted trajectory.\n",
    "- `pred_value_ctrl`: The predicted value for control with shape `(batch_size, 1)`.\n",
    "- `pred_features_ctrl`: The predicted features for control with shape `(batch_size, feature_dim)`.\n",
    "- `mu_branches`: The predicted mean of the control distribution with shape `(batch_size, dim_out)`.\n",
    "- `sigma_branches`: The predicted standard deviation of the control distribution with shape `(batch_size, dim_out)`.\n",
    "- `future_feature`: The predicted future features with shape `(pred_len, batch_size, feature_dim)`.\n",
    "- `future_mu`: The predicted future mean of the control distribution with shape `(pred_len, batch_size, dim_out)`.\n",
    "- `future_sigma`: The predicted future standard deviation of the control distribution with shape `(pred_len, batch_size, dim_out)`.\n",
    "\n",
    "## Code Snippets\n",
    "\n",
    "Here are a few code snippets that highlight important aspects of the TCP_JEPA module:\n",
    "\n",
    "1. Loading the JEPA encoder:\n",
    "\n",
    "```python\n",
    "self.perception = PerceptWrapper(\n",
    "    JepaEncoder.load_model(\n",
    "        config_file_path=jepa_config,\n",
    "        device=self.config.__dict__.get(\"jepa_device\") \n",
    "    ),\n",
    "    \"embed_image\",\n",
    ")\n",
    "```\n",
    "\n",
    "2. Attentive pooling of image features:\n",
    "\n",
    "```python\n",
    "self.attn_pool = AttentivePooler(\n",
    "    num_queries=1,\n",
    "    embed_dim=jepa_embed_dim,\n",
    "    num_heads=8,\n",
    "    mlp_ratio=4.0,\n",
    "    depth=1,\n",
    ")\n",
    "```\n",
    "\n",
    "3. Trajectory prediction using GRU:\n",
    "\n",
    "```python\n",
    "for _ in range(self.config.pred_len):\n",
    "    x_in = torch.cat([x, target_point], dim=1)\n",
    "    z = self.decoder_traj(x_in, z)\n",
    "    traj_hidden_state.append(z)\n",
    "    dx = self.output_traj(z)\n",
    "    x = dx + x\n",
    "    output_wp.append(x)\n",
    "```\n",
    "\n",
    "4. Control prediction using GRU:\n",
    "\n",
    "```python\n",
    "for _ in range(self.config.pred_len):\n",
    "    x_in = torch.cat([x, mu, sigma], dim=1)\n",
    "    h = self.decoder_ctrl(x_in, h)\n",
    "    wp_att = self.wp_att(torch.cat([h, traj_hidden_state[:, _]], 1))\n",
    "    new_feature_emb = torch.bmm(img_embedding, wp_att.unsqueeze(1).transpose(1, 2)).squeeze(dim=-1)\n",
    "    merged_feature = torch.cat([h, new_feature_emb], 1)\n",
    "    dx = self.output_ctrl(merged_feature)\n",
    "    x = dx + x\n",
    "    policy = self.policy_head(x)\n",
    "    mu = self.dist_mu(policy)\n",
    "    sigma = self.dist_sigma(policy)\n",
    "    future_feature.append(x)\n",
    "    future_mu.append(mu)\n",
    "    future_sigma.append(sigma)\n",
    "```\n",
    "\n",
    "## Possible Code Changes\n",
    "\n",
    "Here are a few possible code changes or improvements that could be considered:\n",
    "\n",
    "1. Freezing the JEPA encoder: If the JEPA encoder is pre-trained and not intended to be fine-tuned, you can freeze its weights by uncommenting the line `# self.perception.backbone.freeze_encoder()`.\n",
    "\n",
    "2. Adjusting the dimensions of the fully connected layers: The dimensions of the fully connected layers in the various components (e.g., `measurements`, `join_traj`, `join_ctrl`) can be adjusted based on the specific requirements of the task or the available computational resources.\n",
    "\n",
    "3. Modifying the autoregressive generation loop: The number of steps in the autoregressive generation loop for trajectory and control prediction is determined by `self.config.pred_len`. This value can be adjusted based on the desired length of the predicted trajectory and control actions.\n",
    "\n",
    "4. Experimenting with different attention mechanisms: The attentive pooler currently uses a single query and applies attention to pool the image features. Different attention mechanisms or a varying number of queries could be explored to potentially improve the feature representation.\n",
    "\n",
    "5. Incorporating additional input modalities: The TCP_JEPA module currently uses an image, state information, and a target point as input. Depending on the available data and the specific requirements of the task, additional input modalities such as lidar, radar, or high-level semantic information could be incorporated to enhance the prediction accuracy.\n",
    "\n",
    "These are just a few examples of possible code changes or improvements. The actual modifications would depend on the specific goals, constraints, and performance requirements of the autonomous driving system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "- TCP is a simple way to start learning about Self Driving Cars\n",
    "- Getting set up with TCP requires minimal effort"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
